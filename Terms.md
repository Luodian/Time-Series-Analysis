1. 回归分析

   ![](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/440px-Linear_regression.svg.png)

   简单来说就是找自变量与因变量之间的函数关系。

2. 最小二乘法

   ![](http://opmza2br0.bkt.clouddn.com/17-7-30/13168279.jpg)

   偏导数求极值的公式以后再推把，链接留在这里。

   监督学习中，如果预测的变量是离散的，我们称其为分类（如决策树，支持向量机等），如果预测的变量是连续的，我们称其为回归。回归分析中，如果只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。对于二维空间线性是一条直线；对于三维空间线性是一个平面，对于多维空间线性是一个超平面。

3. 白噪音

   理想的白噪声具有无限[带宽](https://baike.baidu.com/item/%E5%B8%A6%E5%AE%BD)，因而其能量是无限大，这在现实世界是不可能存在的。实际上，我们常常将有限带宽的平整讯号视为白噪音，因为这让我们在[数学分析](https://baike.baidu.com/item/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90)上更加方便。

4. 协方差

   ![](http://opmza2br0.bkt.clouddn.com/17-8-2/59026155.jpg)

   从直观上来看，协方差表示的是两个变量总体误差的期望。

   如果两个变量的变化趋势一致，也就是说如果其中一个大于自身的期望值时另外一个也大于自身的期望值，那么两个变量之间的协方差就是正值；如果两个变量的变化趋势相反，即其中一个变量大于自身的期望值时另外一个却小于自身的期望值，那么两个变量之间的协方差就是负值。

   如果*X*与*Y*是统计独立的，那么二者之间的协方差就是0，因为两个独立的随机变量满足*E*[*XY*]=*E*[*X*]*E*[*Y*]。

   **但是，反过来并不成立。即如果*X*与*Y*的协方差为0，二者并不一定是统计独立的。**

   协方差*Cov*(*X*,*Y*)的度量单位是*X*的协方差乘以*Y*的协方差。

   而取决于协方差的相关性，是一个衡量[线性独立](https://baike.baidu.com/item/%E7%BA%BF%E6%80%A7%E7%8B%AC%E7%AB%8B)的[无量纲](https://baike.baidu.com/item/%E6%97%A0%E9%87%8F%E7%BA%B2)的数。

   协方差为0的两个[随机变量](https://baike.baidu.com/item/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F)称为是不相关的，但仍然注意并不是独立的。

   ​

   ​